# Product Scraper â€“ Serveurs, Stockage, Imprimantes & Scanners

SystÃ¨me de scraping automatisÃ© (multiâ€‘marques) avec postâ€‘traitement IA (Gemini) et insertion MySQL. ConÃ§u pour Ãªtre rapide, modulable, et sÃ»r (idempotent). 

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org) [![Selenium](https://img.shields.io/badge/Selenium-WebDriver-green.svg)](https://selenium.dev) [![AI](https://img.shields.io/badge/AI-Gemini-orange.svg)](https://ai.google.dev) [![License](https://img.shields.io/badge/License-MIT-red.svg)](LICENSE)

## ðŸš€ Principales fonctionnalitÃ©s
- Orchestrateur central (filtrage par catÃ©gories/scripts) + variables d'environnement
- Extraction hybride: tuiles produits (DOM structurÃ©) + JSONâ€‘LD + fallback PDP (enrichissement conditionnel)
- Normalisation IA (Gemini) avec batching, retries, contraintes de format JSON strict
- Upsert MySQL idempotent (clÃ©s uniques `(brand, sku)` & `(brand, link_hash)`), dÃ©sactivation sÃ©lective (`ENABLE_DEACTIVATE_MISSING`)
- Flags de performance: `FAST_SCRAPE`, `HEADLESS_MODE`, `MAX_PRODUCTS`, `SKIP_PDP_ENRICH`
- Journalisation par script et artefacts JSON bruts / nettoyÃ©s
- CLI base de donnÃ©es (`database/db_cli.py`) pour test, listing, export, statistiques

## ðŸ§© PÃ©rimÃ¨tre actuel
| Domaine        | Marques / Scripts |
|----------------|-------------------|
| Serveurs       | ASUS, Dell, HP (tile extractor + PDP fallback), Lenovo, XFusion |
| Stockage       | Dell, Lenovo |
| Imprimantes & Scanners | Epson (Printers + Scanner), HP |

## ðŸ—ï¸ Architecture (vue rapide)
```
ai_processing/        â†’ Nettoyage & politiques Gemini
automation/           â†’ Scheduler / orchestration
database/             â†’ Connexion, schÃ©ma, CLI, migrations lÃ©gÃ¨res
serveurs/, stockage/, imprimantes_scanners/  â†’ Scrapers spÃ©cialisÃ©s
logs/                 â†’ Journaux d'exÃ©cution
```

## âš™ï¸ Installation
```powershell
git clone https://github.com/wassim100/product-scraper.git
cd product-scraper
python -m venv venv
venv\Scripts\Activate
pip install -r requirements.txt
```

CrÃ©er un fichier `.env` (optionnel si valeurs par dÃ©faut) :
```env
GEMINI_API_KEY=VOTRE_CLE
MYSQL_HOST=localhost
MYSQL_USER=root
MYSQL_PASSWORD=
MYSQL_DATABASE=scraping_db
```

## ðŸ”‘ Variables d'environnement principales
| Variable | RÃ´le |
|----------|------|
| HEADLESS_MODE | ExÃ©cution Chrome sans interface |
| FAST_SCRAPE | Active optimisations (timeouts courts, images dÃ©sactivÃ©es) |
| MAX_PRODUCTS | Limite par run (0 = illimitÃ©) |
| ENABLE_DB | Active insertion DB |
| ENABLE_AI_CLEANING | Active nettoyage Gemini post-scrape (scheduler) |
| ENABLE_DEACTIVATE_MISSING | DÃ©sactive en base les produits non revus dans le run |
| SKIP_PDP_ENRICH | Saute l'enrichissement PDP (HP) pour accÃ©lÃ©rer |
| SCHEDULER_CATEGORIES | Filtre (serveurs,stockage,imprimantes_scanners) |
| SCHEDULER_SCRIPTS | Liste prÃ©cise de scripts Ã  exÃ©cuter |
| GEMINI_API_KEY | ClÃ© API Gemini |

## ðŸ§ª ExÃ©cution rapide (exemples)
Scraper 5 serveurs HP en mode rapide :
```powershell
$env:MAX_PRODUCTS=5; $env:FAST_SCRAPE="1"; $env:HEADLESS_MODE="1"; python .\serveurs\hp.py
```

Nettoyage IA manuel :
```powershell
python .\ai_processing\gemini_cleaning.py --in .\hp_servers_full.json --out .\hp_servers_full.cleaned.json --batch-size 2
```

Insertion DB (upsert) :
```powershell
$env:ENABLE_DEACTIVATE_MISSING="false"; python -c "from database.mysql_connector import save_to_database; print(save_to_database('hp_servers_full.cleaned.json','serveurs','HP'))"
```

Scheduler (catÃ©gorie imprimantes & scanners uniquement) :
```powershell
$env:SCHEDULER_CATEGORIES="imprimantes_scanners"; python -m automation.scheduler
```

## ðŸ›¢ï¸ SchÃ©ma & DB
- ClÃ©s uniques : `(brand, sku)` et `(brand, link_hash)`
- Champs lifecycle: `is_active`, `scraped_at`, `last_seen`, `ai_processed` / `ai_processed_at`
- DÃ©sactivation conditionnelle contrÃ´lÃ©e par `ENABLE_DEACTIVATE_MISSING`

## ðŸ¤– IA (Gemini)
Flux : JSON brut â†’ nettoyage (fusion specs / suppression bruit / normalisation clÃ©s) â†’ `.cleaned.json` â†’ DB.
Gestion : lots (`--batch-size`), limite (`--limit`), robustesse (retry basique).

## ðŸ§¹ QualitÃ© / Robustesse
- Extraction HP refactorisÃ©e (tuiles â†’ hints consolidÃ©s â†’ JSON-LD â†’ fallback PDP ciblÃ©)
- `try/finally` systÃ©matique pour fermeture navigateur
- Image-blocking en mode `FAST_SCRAPE`
- Normalisation specs (regex CPU / cores / RAM / stockage / PSU)

## ðŸ” CLI Base de DonnÃ©es
Exemples :
```powershell
python -m database.db_cli test
python -m database.db_cli list --table serveurs --brand HP --limit 5
python -m database.db_cli brands --table serveurs
python -m database.db_cli export --table serveurs --brand HP --out hp_export.json
```

## ðŸ“ Fichiers ignorÃ©s (sÃ©curitÃ© & propretÃ©)
Le `.gitignore` exclut : logs, drivers, artefacts volumineux (`*_full.json`, fichiers `.cleaned.json`), environnements virtuels, secrets `.env`.

## ðŸ“ Licence
MIT â€“ voir `LICENSE`.

## âœ… RÃ©sumÃ© des atouts
> Pipeline complet scrape â†’ enrichissement conditionnel â†’ nettoyage IA â†’ upsert MySQL, modulaire, performant et traÃ§able.

## â­ Contribution
PRs bienvenues : crÃ©ez une branche, dÃ©veloppez, testez, ouvrez une Pull Request.

---
Si ce projet vous est utile, une Ã©toile GitHub est apprÃ©ciÃ©e.

---

_Documentation finale consolidÃ©e â€“ version stable._
### ðŸŽ¯ Extraction multiâ€‘marques
- Serveurs: ASUS, Dell, HP, Lenovo, XFusion
- Stockage: Dell, Lenovo
- Imprimantes & Scanners: Epson (EpsonPrinters + EpsonScanner), HP

### ðŸ”§ FonctionnalitÃ©s avancÃ©es
- Extraction des spÃ©cifications dÃ©taillÃ©es depuis les pages produits
- Pagination robuste + gestion des popups/cookies
- Nettoyage IA automatique (Gemini) aprÃ¨s chaque scraping
- Insertion MySQL avec clÃ©s uniques, suivi lifecycle et dÃ©sactivation des produits non revus
- Orchestrateur unique avec logs par script, timeouts et filtres par catÃ©gorie/script

### ðŸ¤– Intelligence Artificielle
- Gemini 1.5: nettoyage/structuration des `tech_specs`
- Fichiers `.cleaned.json` gÃ©nÃ©rÃ©s puis insÃ©rÃ©s en base (prÃ©fÃ©rÃ©s aux `.json` bruts)

## ðŸ—ï¸ Structure du projet

```
product-scraper/
â”‚
â”œâ”€â”€ ðŸ“ serveurs/                    # Scrapers serveurs (asus/dell/hp/lenovo/xfusion)
â”‚   â”œâ”€â”€ asus.py
â”‚   â”œâ”€â”€ dell.py
â”‚   â”œâ”€â”€ hp.py
â”‚   â”œâ”€â”€ lenovo.py
â”‚   â””â”€â”€ xfusion.py
â”‚
â”œâ”€â”€ ðŸ“ ai_processing/              # Traitement IA
â”‚   â””â”€â”€ gemini_cleaning.py         # Nettoyage Gemini AI
â”‚
â”œâ”€â”€ ðŸ“ database/                   # Base de donnÃ©es
â”‚   â”œâ”€â”€ config.py                  # ParamÃ¨tres MySQL
â”‚   â”œâ”€â”€ mysql_connector.py         # Connecteur + crÃ©ation/migrations simples
â”‚   â””â”€â”€ test_mysql.py              # Test de connexion
â”‚
â”œâ”€â”€ ðŸ“ automation/                 # Automatisation
â”‚   â””â”€â”€ scheduler.py               # Planificateur de tÃ¢ches
â”‚
â”œâ”€â”€ ðŸ“ stockage/                   # Scrapers stockage
â”‚   â”œâ”€â”€ dell.py
â”‚   â””â”€â”€ lenovo.py
â”‚
â”œâ”€â”€ ðŸ“ imprimantes_scanners/       # Imprimantes & scanners
â”‚   â”œâ”€â”€ EpsonPrinters.py           # Epson (imprimantes)
â”‚   â”œâ”€â”€ EpsonScanner.py            # Epson (scanners)
â”‚   â””â”€â”€ hp.py                      # HP
â”‚
â”œâ”€â”€ ðŸ“„ main.py                     # Script principal
â”œâ”€â”€ ðŸ“„ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ ðŸ“„ README.md                   # Documentation
â””â”€â”€ ðŸ“„ .gitignore                  # Fichiers Ã  ignorer
```

## ðŸš€ Installation (Windows)

### 1. **Cloner le repository**
```powershell
git clone https://github.com/wassim100/product-scraper.git
cd product-scraper
```

### 2. **CrÃ©er un environnement virtuel**
```powershell
python -m venv venv
venv\Scripts\Activate
```

### 3. **Installer les dÃ©pendances**
```powershell
pip install -r requirements.txt
```

### 4. ChromeDriver
- Optionnel: placez `chromedriver.exe` Ã  la racine. Sinon Selenium Manager rÃ©soudra automatiquement.
>>>>>>> 2b8329a (feat(epson): add EpsonPrinters and EpsonScanner scrapers; orchestrator DB flow; brand-scoped deactivation flag; README overhaul)

## Troubleshooting

<<<<<<< HEAD
- Script names must match scheduler entries (EpsonPrinters.py, EpsonScanner.py)
- For partial tests: set MAX_PRODUCTS>0 and ENABLE_DEACTIVATE_MISSING=false
- Git: ensure local main tracks origin/main before pushing

## License

MIT
=======
### Orchestrateur (recommandÃ©)
ExÃ©cuter tous les scrapers via le scheduler, avec filtres optionnels:

```powershell
# ExÃ©cution manuelle unique
$env:HEADLESS_MODE="true"; $env:ENABLE_DB="true"; $env:ENABLE_AI_CLEANING="true"; `
$env:SCHEDULER_CATEGORIES="imprimantes_scanners"; `
$env:SCHEDULER_SCRIPTS="imprimantes_scanners/EpsonPrinters.py,imprimantes_scanners/EpsonScanner.py"; `
python .\main.py --mode schedule --manual-run
```

Flags utiles:
- HEADLESS_MODE=true|false
- ENABLE_DB=true|false (insertion DB par le scheduler)
- ENABLE_AI_CLEANING=true|false (nettoyage Gemini postâ€‘scrape)
- ENABLE_DEACTIVATE_MISSING=true|false (dÃ©sactivation des produits non revus)
- MAX_PRODUCTS=10 (run rÃ©duit de test)
- SCHEDULER_CATEGORIES=serveurs,stockage,imprimantes_scanners
- SCHEDULER_SCRIPTS=chemins,relatifs,aux,scripts

Astuce tests: pour Ã©viter de dÃ©sactiver des produits lors dâ€™un run rÃ©duit (MAX_PRODUCTS>0), dÃ©finissez `ENABLE_DEACTIVATE_MISSING=false`.

### ExÃ©cution directe dâ€™un scraper (dÃ©veloppement)
```powershell
python .\imprimantes_scanners\EpsonPrinters.py
python .\imprimantes_scanners\EpsonScanner.py
```

### Traitement IA (manuel)
```powershell
python .\ai_processing\gemini_cleaning.py --in path\to\raw.json --out path\to\cleaned.json
```

## ðŸ“¦ DonnÃ©es extraites

### **Format JSON Standard**
```json
{
  "brand": "XFusion",
  "category": "AI Servers",
  "name": "FusionServer G5500 V7",
  "link": "https://www.xfusion.com/...",
  "tech_specs": {
    "Form Factor": "4U AI server",
    "Processor": "2 x 4th/5th Gen Intel Xeon Scalable",
    "Memory": "32 x DIMMs up to 5600 MT/s"
  },
  "scraped_at": "2025-07-22T10:43:36.808174",
  "datasheet_link": "https://www.xfusion.com/en/resource/...",
  "image_url": "https://www.xfusion.com/wp-content/uploads/..."
}
```

## ðŸ”§ Configuration

### **Variables d'environnement (.env)**
```env
# API
GEMINI_API_KEY=your_gemini_api_key_here

# MySQL (optionnel si valeurs par dÃ©faut)
MYSQL_HOST=localhost
MYSQL_USER=root
MYSQL_PASSWORD=
MYSQL_DATABASE=scraping_db

# Scraping / Orchestrateur
HEADLESS_MODE=true
ENABLE_DB=true
ENABLE_AI_CLEANING=true
ENABLE_DEACTIVATE_MISSING=true
MAX_PRODUCTS=0
SCHEDULER_CATEGORIES=
SCHEDULER_SCRIPTS=
```

## ðŸŽ¯ DÃ©tails notables
- ClÃ©s uniques DB: (brand, sku) et (brand, link_hash) pour dÃ©duplication fiable
- Champs lifecycle: is_active, last_seen + audit IA (ai_processed, ai_processed_at)
- Le scheduler force RUNNING_UNDER_SCHEDULER=1 pour Ã©viter les doubles insertions cÃ´tÃ© scrapers

## ðŸ§ª Tests & validation
- Validez par runs rÃ©duits: `MAX_PRODUCTS=1` + `ENABLE_DEACTIVATE_MISSING=false`
- Consultez les logs par script dans `./logs/` et le rapport JSON dâ€™exÃ©cution du scheduler

## ðŸ“ˆ Bonnes pratiques
- PrÃ©fÃ©rez les `.cleaned.json` pour lâ€™insertion DB
- Ã‰vitez les runs rÃ©duits avec dÃ©sactivation active
- Placez le dossier sur un chemin court si OneDrive verrouille des fichiers

## ðŸ› ï¸ Technologies utilisÃ©es

- **Python 3.8+**
- **Selenium WebDriver** : Automation web
- **Google Gemini AI** : Nettoyage structurÃ© des spÃ©cifications
- **MySQL** : Stockage base de donnÃ©es (optionnel)
- **JSON** : Format de donnÃ©es principal
- **Chrome/Chromium** : Navigateur pour scraping

## ðŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit les changements (`git commit -am 'Ajout nouvelle fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. CrÃ©er une Pull Request

## ðŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ðŸ™ Remerciements

- **Selenium** pour l'automation web
- **Google Gemini AI** pour le traitement intelligent
- **ChromeDriver** pour l'exÃ©cution des navigateurs

## ðŸ“ž Contact

- **Auteur** : Wassim
- **GitHub** : [wassim100](https://github.com/wassim100)
- **Repository** : [product-scraper](https://github.com/wassim100/product-scraper)

---

â­ **N'hÃ©sitez pas Ã  mettre une Ã©toile si ce projet vous aide !** â­
>>>>>>> 2b8329a (feat(epson): add EpsonPrinters and EpsonScanner scrapers; orchestrator DB flow; brand-scoped deactivation flag; README overhaul)
