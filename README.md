<<<<<<< HEAD
# Selenium Scraper Suite

Production-ready scraping suite for servers, storage, and printers/scanners. It orchestrates Selenium scrapers, optional Gemini-based cleaning, and MySQL storage via a central scheduler.

## Highlights
=======
# Product Scraper â€“ Serveurs, Stockage, Imprimantes & Scanners

SystÃ¨me de scraping automatisÃ© avec postâ€‘traitement IA (Gemini) et insertion MySQL, orchestrÃ© par un scheduler unique.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![Selenium](https://img.shields.io/badge/Selenium-WebDriver-green.svg)](https://selenium.dev)
[![AI](https://img.shields.io/badge/AI-Gemini-orange.svg)](https://ai.google.dev)
[![License](https://img.shields.io/badge/License-MIT-red.svg)](LICENSE)
>>>>>>> 2b8329a (feat(epson): add EpsonPrinters and EpsonScanner scrapers; orchestrator DB flow; brand-scoped deactivation flag; README overhaul)

- Central scheduler (filter by categories or scripts)
- Gemini cleaning (optional) with batching and retries
- MySQL upserts with brand-scoped deactivate-missing
- Safe testing flags: headless, max products, deactivate toggle
- Per-script logs and JSON artifacts

<<<<<<< HEAD
## Setup

1) Python 3.11+ (3.12 recommended)
2) pip install -r requirements.txt
3) Create .env with DB creds and GEMINI_API_KEY

## Run via scheduler

- All: python -m automation.scheduler
- Only printers/scanners: set SCHEDULER_CATEGORIES=imprimantes_scanners
- Only Epson scripts: set SCHEDULER_SCRIPTS=EpsonPrinters.py,EpsonScanner.py

Env flags:
- HEADLESS_MODE=true|false (default true)
- ENABLE_DB=true|false (default true)
- ENABLE_AI_CLEANING=true|false (default true)
- ENABLE_DEACTIVATE_MISSING=true|false (default true)
- MAX_PRODUCTS=0 (0 = no limit)
- SCHEDULER_CATEGORIES=serveurs,stockage,imprimantes_scanners
- SCHEDULER_SCRIPTS=comma-separated names
- MYSQL_HOST, MYSQL_PORT, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE
- GEMINI_API_KEY

## Epson scripts

- imprimantes_scanners/EpsonPrinters.py â†’ epson_printers_scanners_full.json
- imprimantes_scanners/EpsonScanner.py â†’ epson_scanners_full.json

When RUNNING_UNDER_SCHEDULER=true, scrapers skip direct DB writes; the scheduler handles AI + DB.

## Notes

- Logs: logs/ per run
- Selenium: Chrome; keep chromedriver.exe or rely on Selenium Manager
- MySQL: Unique (brand, sku) and (brand, link_hash); lifecycle fields present
- AI cleaning: see ai_processing/gemini_cleaning.py
=======
### ðŸŽ¯ Extraction multiâ€‘marques
- Serveurs: ASUS, Dell, HP, Lenovo, XFusion
- Stockage: Dell, Lenovo
- Imprimantes & Scanners: Epson (EpsonPrinters + EpsonScanner), HP

### ðŸ”§ FonctionnalitÃ©s avancÃ©es
- Extraction des spÃ©cifications dÃ©taillÃ©es depuis les pages produits
- Pagination robuste + gestion des popups/cookies
- Nettoyage IA automatique (Gemini) aprÃ¨s chaque scraping
- Insertion MySQL avec clÃ©s uniques, suivi lifecycle et dÃ©sactivation des produits non revus
- Orchestrateur unique avec logs par script, timeouts et filtres par catÃ©gorie/script

### ðŸ¤– Intelligence Artificielle
- Gemini 1.5: nettoyage/structuration des `tech_specs`
- Fichiers `.cleaned.json` gÃ©nÃ©rÃ©s puis insÃ©rÃ©s en base (prÃ©fÃ©rÃ©s aux `.json` bruts)

## ðŸ—ï¸ Structure du projet

```
product-scraper/
â”‚
â”œâ”€â”€ ðŸ“ serveurs/                    # Scrapers serveurs (asus/dell/hp/lenovo/xfusion)
â”‚   â”œâ”€â”€ asus.py
â”‚   â”œâ”€â”€ dell.py
â”‚   â”œâ”€â”€ hp.py
â”‚   â”œâ”€â”€ lenovo.py
â”‚   â””â”€â”€ xfusion.py
â”‚
â”œâ”€â”€ ðŸ“ ai_processing/              # Traitement IA
â”‚   â””â”€â”€ gemini_cleaning.py         # Nettoyage Gemini AI
â”‚
â”œâ”€â”€ ðŸ“ database/                   # Base de donnÃ©es
â”‚   â”œâ”€â”€ config.py                  # ParamÃ¨tres MySQL
â”‚   â”œâ”€â”€ mysql_connector.py         # Connecteur + crÃ©ation/migrations simples
â”‚   â””â”€â”€ test_mysql.py              # Test de connexion
â”‚
â”œâ”€â”€ ðŸ“ automation/                 # Automatisation
â”‚   â””â”€â”€ scheduler.py               # Planificateur de tÃ¢ches
â”‚
â”œâ”€â”€ ðŸ“ stockage/                   # Scrapers stockage
â”‚   â”œâ”€â”€ dell.py
â”‚   â””â”€â”€ lenovo.py
â”‚
â”œâ”€â”€ ðŸ“ imprimantes_scanners/       # Imprimantes & scanners
â”‚   â”œâ”€â”€ EpsonPrinters.py           # Epson (imprimantes)
â”‚   â”œâ”€â”€ EpsonScanner.py            # Epson (scanners)
â”‚   â””â”€â”€ hp.py                      # HP
â”‚
â”œâ”€â”€ ðŸ“„ main.py                     # Script principal
â”œâ”€â”€ ðŸ“„ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ ðŸ“„ README.md                   # Documentation
â””â”€â”€ ðŸ“„ .gitignore                  # Fichiers Ã  ignorer
```

## ðŸš€ Installation (Windows)

### 1. **Cloner le repository**
```powershell
git clone https://github.com/wassim100/product-scraper.git
cd product-scraper
```

### 2. **CrÃ©er un environnement virtuel**
```powershell
python -m venv venv
venv\Scripts\Activate
```

### 3. **Installer les dÃ©pendances**
```powershell
pip install -r requirements.txt
```

### 4. ChromeDriver
- Optionnel: placez `chromedriver.exe` Ã  la racine. Sinon Selenium Manager rÃ©soudra automatiquement.
>>>>>>> 2b8329a (feat(epson): add EpsonPrinters and EpsonScanner scrapers; orchestrator DB flow; brand-scoped deactivation flag; README overhaul)

## Troubleshooting

<<<<<<< HEAD
- Script names must match scheduler entries (EpsonPrinters.py, EpsonScanner.py)
- For partial tests: set MAX_PRODUCTS>0 and ENABLE_DEACTIVATE_MISSING=false
- Git: ensure local main tracks origin/main before pushing

## License

MIT
=======
### Orchestrateur (recommandÃ©)
ExÃ©cuter tous les scrapers via le scheduler, avec filtres optionnels:

```powershell
# ExÃ©cution manuelle unique
$env:HEADLESS_MODE="true"; $env:ENABLE_DB="true"; $env:ENABLE_AI_CLEANING="true"; `
$env:SCHEDULER_CATEGORIES="imprimantes_scanners"; `
$env:SCHEDULER_SCRIPTS="imprimantes_scanners/EpsonPrinters.py,imprimantes_scanners/EpsonScanner.py"; `
python .\main.py --mode schedule --manual-run
```

Flags utiles:
- HEADLESS_MODE=true|false
- ENABLE_DB=true|false (insertion DB par le scheduler)
- ENABLE_AI_CLEANING=true|false (nettoyage Gemini postâ€‘scrape)
- ENABLE_DEACTIVATE_MISSING=true|false (dÃ©sactivation des produits non revus)
- MAX_PRODUCTS=10 (run rÃ©duit de test)
- SCHEDULER_CATEGORIES=serveurs,stockage,imprimantes_scanners
- SCHEDULER_SCRIPTS=chemins,relatifs,aux,scripts

Astuce tests: pour Ã©viter de dÃ©sactiver des produits lors dâ€™un run rÃ©duit (MAX_PRODUCTS>0), dÃ©finissez `ENABLE_DEACTIVATE_MISSING=false`.

### ExÃ©cution directe dâ€™un scraper (dÃ©veloppement)
```powershell
python .\imprimantes_scanners\EpsonPrinters.py
python .\imprimantes_scanners\EpsonScanner.py
```

### Traitement IA (manuel)
```powershell
python .\ai_processing\gemini_cleaning.py --in path\to\raw.json --out path\to\cleaned.json
```

## ðŸ“¦ DonnÃ©es extraites

### **Format JSON Standard**
```json
{
  "brand": "XFusion",
  "category": "AI Servers",
  "name": "FusionServer G5500 V7",
  "link": "https://www.xfusion.com/...",
  "tech_specs": {
    "Form Factor": "4U AI server",
    "Processor": "2 x 4th/5th Gen Intel Xeon Scalable",
    "Memory": "32 x DIMMs up to 5600 MT/s"
  },
  "scraped_at": "2025-07-22T10:43:36.808174",
  "datasheet_link": "https://www.xfusion.com/en/resource/...",
  "image_url": "https://www.xfusion.com/wp-content/uploads/..."
}
```

## ðŸ”§ Configuration

### **Variables d'environnement (.env)**
```env
# API
GEMINI_API_KEY=your_gemini_api_key_here

# MySQL (optionnel si valeurs par dÃ©faut)
MYSQL_HOST=localhost
MYSQL_USER=root
MYSQL_PASSWORD=
MYSQL_DATABASE=scraping_db

# Scraping / Orchestrateur
HEADLESS_MODE=true
ENABLE_DB=true
ENABLE_AI_CLEANING=true
ENABLE_DEACTIVATE_MISSING=true
MAX_PRODUCTS=0
SCHEDULER_CATEGORIES=
SCHEDULER_SCRIPTS=
```

## ðŸŽ¯ DÃ©tails notables
- ClÃ©s uniques DB: (brand, sku) et (brand, link_hash) pour dÃ©duplication fiable
- Champs lifecycle: is_active, last_seen + audit IA (ai_processed, ai_processed_at)
- Le scheduler force RUNNING_UNDER_SCHEDULER=1 pour Ã©viter les doubles insertions cÃ´tÃ© scrapers

## ðŸ§ª Tests & validation
- Validez par runs rÃ©duits: `MAX_PRODUCTS=1` + `ENABLE_DEACTIVATE_MISSING=false`
- Consultez les logs par script dans `./logs/` et le rapport JSON dâ€™exÃ©cution du scheduler

## ðŸ“ˆ Bonnes pratiques
- PrÃ©fÃ©rez les `.cleaned.json` pour lâ€™insertion DB
- Ã‰vitez les runs rÃ©duits avec dÃ©sactivation active
- Placez le dossier sur un chemin court si OneDrive verrouille des fichiers

## ðŸ› ï¸ Technologies utilisÃ©es

- **Python 3.8+**
- **Selenium WebDriver** : Automation web
- **Google Gemini AI** : Nettoyage structurÃ© des spÃ©cifications
- **MySQL** : Stockage base de donnÃ©es (optionnel)
- **JSON** : Format de donnÃ©es principal
- **Chrome/Chromium** : Navigateur pour scraping

## ðŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit les changements (`git commit -am 'Ajout nouvelle fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. CrÃ©er une Pull Request

## ðŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ðŸ™ Remerciements

- **Selenium** pour l'automation web
- **Google Gemini AI** pour le traitement intelligent
- **ChromeDriver** pour l'exÃ©cution des navigateurs

## ðŸ“ž Contact

- **Auteur** : Wassim
- **GitHub** : [wassim100](https://github.com/wassim100)
- **Repository** : [product-scraper](https://github.com/wassim100/product-scraper)

---

â­ **N'hÃ©sitez pas Ã  mettre une Ã©toile si ce projet vous aide !** â­
>>>>>>> 2b8329a (feat(epson): add EpsonPrinters and EpsonScanner scrapers; orchestrator DB flow; brand-scoped deactivation flag; README overhaul)
